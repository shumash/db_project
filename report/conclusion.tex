\section{Conclusion}\label{sec:conclusion}

We have presented in this paper a largely scalable lossy compression scheme which exploits indexing infrastructure provided by database management systems for efficient storage, and exploits the redundancy of large image databases to drastically reduce the storage cost.  In doing so, we compared three methods which exploited locality sensitive hashing for image indexing - a naive method which relied on random projections, a method which exploited the dimensionality reduction of principal component analysis, and a second PCA-based method which also aimed to make the storage process more efficient and provide qualitative image quality improvements by recognizing uniform patches in LUV color-space.  Our experiments demonstrated that the cost of image insertion for our PCA-based hashing grows at a much slower rate than the naive method.  Meanwhile, qualitative evaluation demonstrated that the reconstruction quality improved with our advanced methods while sacrificing little in additional patch storage.  Our methods allowed for patch matching far faster than brute force methods, which early experiments proved to be infeasibly slow.

A few open questions remain.  Although most insertion strategies are likely bounded by a quadratic asymptotic runtime, it remains an open question what the \emph{expected} runtime of our hashing based insertion methods - although we empirically see the cost of each additional image scaling roughly linearly in database size, the trend remains to be proven.  While our insertion method is much faster than the brute force method, which we found to be intractably slow for our purposes,  it is an open question if a method exists which can still provide good compression and database growth guarantees while having a constant insertion cost.

Finally, although we saw storage improvements in a regime of 10,000 images with good compression quality, we anticipate these trends to be more pronounced for very large databases of ~10,000,000 images or more. More experiments still need to be run to validate this belief empirically.

\input{apps.tex}


\subsection{Future Extensions}
\label{sec:futureext}

In this paper we considered a simple implementation of a patch-based image database compression scheme, where the images and patches were square and of fixed sizes. Patches were sampled in a regular, non-overlapping grid from each image.  Alternative approaches include more flexible, context-aware, patch-sampling techniques. For instance, the patch granularity for sampling large homogenous sky and field regions may be different from the one used for sampling highly-textured regions like objects and structures (trees, buildings, people, etc.). Similarly, patches that do not cross object boundaries are likely to lead to less artifacts in future reconstructions. For this, approaches like Selective Search \cite{UijlingsIJCV2013} that localize image regions likely to contain objects, may prove promising for sampling patches.

If patches were different sizes, then one of a number of extensions to the system would be required - for instance, (1) a patch transformation scheme, or (2) a hierarchical patch dictionary. A patch transformation scheme would permit each patch to be transformed (in a simple way - e.g. via rescaling) to match another patch with the same appearance but different (scale) parameters. For instance, a small patch in one image may be sufficient to account for a much larger part of another image, and rather than storing many separate patches of different sizes, we would benefit from quickly applying transformations to existing dictionary patches. To implement this system would require storing, for each image location, not only a pointer to a patch in the patch dictionary but also a transformation (e.g. a set of scaling parameters). Naturally, the cost function (to weigh the benefits of such a scheme vs storing the original images or even equally-sized patches) would need to take into account (a) the extra parameters stored along with each image location, and (b) the reconstruction time overhead for patch transformation. With large enough datasets, this approach may be effective at eliminating redundancy.

Another approach, building hierarchical patch dictionaries, may speed up the patchifying and subsequent reconstruction of an image, by offering a top-down approach. If larger patches match, there is no need to parse the image at a finer-grained scale. Only if large patches do not properly account for the structure in an image, would it be necessary to go to a finer-grained patch size. Note that small patches could be composed into larger patches, via a hierarchy, so that if large-patch matches are not found, descending down the patch hierarchy of the best-matching large patches would make it possible to find a match at a lower granularity. This scheme would be less flexible than the patch transformation scheme, but may prove to be more efficient. 

