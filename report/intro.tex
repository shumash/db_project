\section{Introduction}

Large collections of images are ubiquitous in the modern digital world.
According to one 2014 Internet Trends report,
more than 1.8 billion images are uploaded to the internet every day~\cite{meeker2014internet}.
Our work is inspired by the intuition that there must be a lot of redundancy
in large image collections, and that this redundancy could
be exploited for more efficient storage and for applications such as duplicate detection.

We focus on image redundancy on the patch level, assuming that large collections
of images must have many patches which are nearly the same.
Our goal is to store a set of images as a database of
similar patches, where similar patches may be shared between images,
 such that we minimize the storage space while maintaining certain \emph{quality}
of reconstructed images. In effect, this results in lossy compression. More concretely,
we aim to choose a patch distance criterion, matching and reconstruction algorithms such
that:
\begin{itemize}
\item the database size is smaller than if full images were stored
\item the images can be reconstructed from the database in real time
\item the reconstructed images fulfill certain quality requirements (see sec.~\ref{sec:analysis})
\end{itemize}
These conflicts introduce a number of tradeoffs, such as size of
the database versus image quality.
The goal of this paper is as much to produce a working system as to
build up the analytical foundations that allow making these tradeoffs.

Section \ref{sec:related} covers some related work on image compression, raster databases, and patch-based computer vision applications.
In section~\ref{sec:method}, we explain our method of ``patchifying'' images,
storing them in a database, and reconstructing stored images. 
In section \ref{sec:nn} we discuss how we accomplish the fast retrieval of similar patches via locality-sensitive-hashing, which is crucial to the feasibility of our system. In section \ref{sec:opt} we discuss other important database optimizations required for performance. 
In section~\ref{sec:analysis}, we provide the analytical groundwork for
selecting optimal image patch sizes, distance thresholds, and quality metrics
appropriate for evaluation. Finally, in sections~\ref{sec:performance} and \ref{sec:qual},
we quantitatively and qualitatively evaluate our full working system on real data, using the analytical tools developed from the previous section. 
In addition, in section~\ref{sec:conclusion}, we briefly touch on applications that derive naturally
from a patch database, including similar image retrieval and duplicate detection, as well as a fun photomosaic application. We conclude by discussing future extensions.
